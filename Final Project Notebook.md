# **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ**

## **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫**

```python
# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
# !pip install -q transformers torch datasets evaluate nltk matplotlib seaborn
```

## **–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞**

```python
import torch
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
from transformers import (
    T5ForConditionalGeneration, T5Tokenizer,
    GPT2LMHeadModel, GPT2Tokenizer,
    AutoModelForQuestionAnswering, AutoTokenizer
)
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.tokenize import word_tokenize
import nltk
import os
from getpass import getpass

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö NLTK
nltk.download('punkt', quiet=True)

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤–≤–æ–¥ —Ç–æ–∫–µ–Ω–∞
HF_API_TOKEN = getpass("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à Hugging Face —Ç–æ–∫–µ–Ω: ")
os.environ["HF_TOKEN"] = HF_API_TOKEN

# –ö—ç—à –¥–ª—è –º–æ–¥–µ–ª–µ–π –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
model_cache = {}
```

## **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**

```python
try:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç SberQuAD
    dataset = load_dataset("kuznetsoffandrey/sberquad", token=HF_API_TOKEN)
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç SberQuAD –∑–∞–≥—Ä—É–∂–µ–Ω! –†–∞–∑–º–µ—Ä: {len(dataset['validation'])} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –í—ã–±–∏—Ä–∞–µ–º 50 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    num_examples = 50
    test_data = dataset["validation"].shuffle(seed=42).select(range(num_examples))
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(test_data)} —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    test_data = None
```

## **–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫**

```python
def simple_tokenize(text):
    """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º."""
    return text.split()

def compute_exact_match(prediction, reference):
    """–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (Exact Match)."""
    return int(prediction.strip() == reference.strip())

def compute_f1(prediction, reference):
    """–†–∞—Å—á—ë—Ç F1 Score –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤."""
    pred_tokens = set(simple_tokenize(prediction.lower()))
    ref_tokens = set(simple_tokenize(reference.lower()))

    if len(pred_tokens) == 0 or len(ref_tokens) == 0:
        return 0

    common_tokens = pred_tokens.intersection(ref_tokens)
    precision = len(common_tokens) / len(pred_tokens)
    recall = len(common_tokens) / len(ref_tokens)

    if precision + recall == 0:
        return 0
    return 2 * (precision * recall) / (precision + recall)

def compute_bleu(prediction, reference):
    """–†–∞—Å—á—ë—Ç BLEU Score —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLTK."""
    smoothing = SmoothingFunction().method1
    pred_tokens = simple_tokenize(prediction.lower())
    ref_tokens = [simple_tokenize(reference.lower())]
    return sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)
```

## **–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏**

```python
def get_model_and_tokenizer(model_name, model_type):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ –∏–º–µ–Ω–∏."""
    if model_name in model_cache:
        return model_cache[model_name]

    try:
        if model_type == "T5":
            tokenizer = T5Tokenizer.from_pretrained(model_name, token=HF_API_TOKEN, legacy=False)
            model = T5ForConditionalGeneration.from_pretrained(model_name, token=HF_API_TOKEN)
        elif model_type == "GPT2":
            tokenizer = GPT2Tokenizer.from_pretrained(model_name, token=HF_API_TOKEN)
            tokenizer.pad_token = tokenizer.eos_token
            model = GPT2LMHeadModel.from_pretrained(model_name, token=HF_API_TOKEN)
        elif model_type == "QA":
            tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_API_TOKEN)
            model = AutoModelForQuestionAnswering.from_pretrained(model_name, token=HF_API_TOKEN)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
            return None, None

        model_cache[model_name] = (model, tokenizer)
        return model, tokenizer
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        return None, None
```

## **–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π**

```python
def test_t5_model_with_params(model_name, test_data, prompt_format="question: {question} context: {context}",
                             temperature=1.0, max_length=50):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç T5-–º–æ–¥–µ–ª—å —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    try:
        model, tokenizer = get_model_and_tokenizer(model_name, "T5")
        if model is None or tokenizer is None:
            return []

        results = []
        for sample in test_data:
            question = sample['question']
            context = sample['context']
            reference = sample['answers']['text'][0]
            start_time = time.time()

            input_text = prompt_format.format(question=question, context=context) if not callable(prompt_format) else prompt_format(sample)
            inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)

            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True if temperature != 1.0 else False
                )

            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
            elapsed_time = time.time() - start_time

            results.append({
                'question': question,
                'context': context,
                'reference': reference,
                'answer': answer,
                'elapsed_time': elapsed_time
            })

        return results
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ test_t5_model_with_params: {e}")
        return []

def test_gpt2_model_with_params(model_name, test_data, prompt_format="–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:",
                               temperature=1.0, max_new_tokens=14):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç GPT2-–º–æ–¥–µ–ª—å."""
    try:
        model, tokenizer = get_model_and_tokenizer(model_name, "GPT2")
        if model is None or tokenizer is None:
            return []

        results = []
        for sample in test_data:
            question = sample['question']
            context = sample['context']
            reference = sample['answers']['text'][0]
            start_time = time.time()

            input_text = prompt_format.format(question=question, context=context) if not callable(prompt_format) else prompt_format(sample)
            inputs = tokenizer(input_text, return_tensors="pt")

            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    num_beams=5,
                    early_stopping=True,
                    no_repeat_ngram_size=2,
                    do_sample=True if temperature != 1.0 else False,
                )

            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = answer[len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):].strip()
            elapsed_time = time.time() - start_time

            results.append({
                'question': question,
                'context': context,
                'reference': reference,
                'answer': answer,
                'elapsed_time': elapsed_time
            })

        return results
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return []

def test_qa_model(model_name, test_data):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º (QA)."""
    try:
        model, tokenizer = get_model_and_tokenizer(model_name, "QA")
        if model is None or tokenizer is None:
            return []

        results = []
        for sample in test_data:
            question = sample['question']
            context = sample['context']
            reference = sample['answers']['text'][0]
            start_time = time.time()

            inputs = tokenizer(question, context, return_tensors="pt", truncation=True, max_length=512)

            with torch.no_grad():
                outputs = model(**inputs)

            answer_start = torch.argmax(outputs.start_logits)
            answer_end = torch.argmax(outputs.end_logits) + 1
            answer_ids = inputs["input_ids"][0][answer_start:answer_end]
            answer = tokenizer.decode(answer_ids, skip_special_tokens=True)
            elapsed_time = time.time() - start_time

            results.append({
                'question': question,
                'context': context,
                'reference': reference,
                'answer': answer,
                'elapsed_time': elapsed_time
            })

        return results
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return []
```

## **–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏**

```python
def evaluate_metrics(results):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å–ø–∏—Å–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    metrics = []
    for result in results:
        prediction = result['answer']
        reference = result['reference']
        elapsed_time = result['elapsed_time']

        metric = {
            "exact_match": compute_exact_match(prediction, reference),
            "f1": compute_f1(prediction, reference),
            "bleu": compute_bleu(prediction, reference),
            "generation_time": elapsed_time,
            "length": len(prediction.split())
        }

        metrics.append(metric)

    return metrics

def print_average_metrics(metrics, title=""):
    """–í—ã–≤–æ–¥–∏—Ç —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫."""
    if not metrics:
        print(f"{title} ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return None

    avg = {
        "exact_match": np.mean([m['exact_match'] for m in metrics]),
        "f1": np.mean([m['f1'] for m in metrics]),
        "bleu": np.mean([m['bleu'] for m in metrics]),
        "generation_time": np.mean([m['generation_time'] for m in metrics]),
        "length": np.mean([m['length'] for m in metrics])
    }

    print(f"{title} –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"  EM: {avg['exact_match']:.4f}")
    print(f"  F1: {avg['f1']:.4f}")
    print(f"  BLEU: {avg['bleu']:.4f}")
    print(f"  –í—Ä–µ–º—è: {avg['generation_time']:.2f} —Å")
    print(f"  –î–ª–∏–Ω–∞: {avg['length']:.2f} —Å–ª–æ–≤\n")

    return avg

def visualize_all_temperature_results(all_results):
    """–ì—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –æ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã."""
    metrics = ['exact_match', 'f1', 'bleu', 'generation_time', 'length']

    for metric in metrics:
        plt.figure(figsize=(10, 5))

        for model, results in all_results.items():
            if model == "QA":
                value = results.get("default", {}).get(metric, 0)
                plt.plot([0.1, 2.0], [value]*2, 'o--', label=f"{model}")
                continue

            temp_points = []
            for key, res in results.items():
                if key.startswith("T="):
                    try:
                        temp = float(key.split("=", 1)[1])
                        value = res.get(metric, 0)
                        temp_points.append((temp, value))
                    except (ValueError, IndexError):
                        continue

            if temp_points:
                temp_points.sort(key=lambda x: x[0])
                temps, values = zip(*temp_points)
                plt.plot(temps, values, 'o-', label=model)

        plt.title(f'{metric.upper()} vs –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞')
        plt.xlabel('–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞')
        plt.ylabel(metric)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
```

## **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã**

```python
def experiment_temperature(model_name, model_type, test_data):
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –≤–ª–∏—è–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
    temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]
    results = {}

    for temp in temperatures:
        print(f"üå°Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã {temp}...")
        if model_type == "T5":
            res = test_t5_model_with_params(model_name, test_data, temperature=temp)
        elif model_type == "GPT2":
            res = test_gpt2_model_with_params(model_name, test_data, temperature=temp)
        else:
            res = []

        metrics = evaluate_metrics(res)
        results[f"T={temp}"] = print_average_metrics(metrics, f"T={temp}: ")

    return results

def analyze_errors(results):
    """–°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫."""
    return [
        {
            "question": r['question'],
            "reference": r['reference'],
            "prediction": r['answer']
        }
        for r in results if r['answer'].strip() != r['reference'].strip()
    ]
```

## **–û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**

```python
if test_data:
    all_results = {}

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ T5
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ T5")
    t5_results = experiment_temperature("cointegrated/rut5-base-multitask", "T5", test_data)
    all_results["T5"] = t5_results

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GPT2
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GPT2")
    gpt2_results = experiment_temperature("sberbank-ai/rugpt3small_based_on_gpt2", "GPT2", test_data)
    all_results["GPT2"] = gpt2_results

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ QA
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ QA")
    qa_results = test_qa_model("Den4ikAI/rubert_large_squad_2", test_data)
    qa_metrics = evaluate_metrics(qa_results)
    qa_avg = print_average_metrics(qa_metrics, "QA: ")
    all_results["QA"] = {"default": qa_avg}

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    visualize_all_temperature_results(all_results)

    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    print("\n–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\n")

    models_to_test = [
        ("cointegrated/rut5-base-multitask", "T5", "T5"),
        ("sberbank-ai/rugpt3small_based_on_gpt2", "GPT2", "GPT2"),
        ("Den4ikAI/rubert_large_squad_2", "QA", "QA")
    ]

    for model_name, model_type, display_name in models_to_test:
        try:
            print(f"üü¢ –ú–æ–¥–µ–ª—å: {display_name} ({model_name})")

            if model_type == "T5":
                examples = test_t5_model_with_params(model_name, test_data, temperature=0.5)
            elif model_type == "GPT2":
                examples = test_gpt2_model_with_params(model_name, test_data, temperature=0.5)
            elif model_type == "QA":
                examples = test_qa_model(model_name, test_data)

            errors = analyze_errors(examples)

            if errors:
                for err in errors[:3]:
                    print(f"  –í–æ–ø—Ä–æ—Å: {err['question']}")
                    print(f"    –û—Ç–≤–µ—Ç: {err['prediction']}")
                    print(f"    –≠—Ç–∞–ª–æ–Ω: {err['reference']}\n")
            else:
                print("  –ù–µ—Ç –æ—à–∏–±–æ–∫ (–≤—Å–µ –æ—Ç–≤–µ—Ç—ã —Å–æ–≤–ø–∞–ª–∏ —Å —ç—Ç–∞–ª–æ–Ω–æ–º)\n")
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}\n")
else:
    print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞.")
```